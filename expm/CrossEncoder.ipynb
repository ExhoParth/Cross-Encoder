{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 Hidden States Shape: tensor([0.0170, 0.0162, 0.0150, 0.0162, 0.0155, 0.0169, 0.0149, 0.0171, 0.0175,\n",
      "        0.0169, 0.0154, 0.0187, 0.0187, 0.0187, 0.0187, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157,\n",
      "        0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157, 0.0157])\n",
      "S2 Hidden States Shape: tensor([0.0178, 0.0174, 0.0162, 0.0179, 0.0201, 0.0165, 0.0172, 0.0162, 0.0193,\n",
      "        0.0174, 0.0167, 0.0157, 0.0170, 0.0201, 0.0201, 0.0201, 0.0201, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182,\n",
      "        0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182, 0.0182])\n",
      "Similarity between 'The cat sits on the mat Machine learning is fascinating Hey Hey Hey' and 'A feline rests on a carpet AI is an exciting field Ha Ha Ha': 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class CrossEncoderCSR(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"roberta-base\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize encoders\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.encoder_config = self.encoder.config\n",
    "        self.total_layers = len(self.encoder.encoder.layer)\n",
    "\n",
    "        self.d_k = self.encoder_config.hidden_size\n",
    "\n",
    "        self.W_pseduo_q = nn.Linear(self.d_k, self.d_k)\n",
    "\n",
    "        self.W_pseduo_k = nn.Linear(self.d_k, self.d_k)\n",
    "\n",
    "    def forward(self, s1_input_ids, s1_attention_mask, s2_input_ids, s2_attention_mask, c_input_ids, c_attention_mask):\n",
    "        # Encode context\n",
    "        c = self.encoder(c_input_ids, c_attention_mask, output_attentions=True)\n",
    "\n",
    "        # [CLS] <- (1, d_c)\n",
    "        cls = c[0][:, 0, :]\n",
    "\n",
    "        q_cls = self.W_pseduo_q(cls)\n",
    "\n",
    "        # print(\"---DEBUG---\")\n",
    "        # print(\"CLS: \", q_cls.shape)\n",
    "\n",
    "        # Start with token embeddings\n",
    "        s_1_h = self.encoder.embeddings(s1_input_ids)\n",
    "        s_2_h = self.encoder.embeddings(s2_input_ids)\n",
    "\n",
    "        # Process through first half of layers\n",
    "        for i in range(self.total_layers // 2):\n",
    "            layer = self.encoder.encoder.layer[i]\n",
    "            s_1_h = layer(s_1_h, attention_mask=s1_attention_mask)[0]\n",
    "            s_2_h = layer(s_2_h, attention_mask=s2_attention_mask)[0]\n",
    "            \n",
    "        k_s_1 = self.W_pseduo_k(s_1_h) \n",
    "        k_s_2 = self.W_pseduo_k(s_2_h)\n",
    "        \n",
    "        # Compute router weights\n",
    "        w_s_1 = self.get_c_router_weights(q_cls, k_s_1)\n",
    "        w_s_2 = self.get_c_router_weights(q_cls, k_s_2)\n",
    "\n",
    "        # Apply weights\n",
    "        s_1_h = (w_s_1.transpose(1, 2) + 1) * s_1_h\n",
    "        s_2_h = (w_s_2.transpose(1, 2) + 1) * s_2_h\n",
    "\n",
    "        # Process through second half of layers\n",
    "        for i in range(self.total_layers // 2, self.total_layers):\n",
    "            layer = self.encoder.encoder.layer[i]\n",
    "            s_1_h = layer(s_1_h, attention_mask=s1_attention_mask)[0]\n",
    "            s_2_h = layer(s_2_h, attention_mask=s2_attention_mask)[0]\n",
    "\n",
    "        # return s_1_hidden_state, s_2_hidden_state\n",
    "\n",
    "        average_pooling = nn.AdaptiveAvgPool1d(1)  \n",
    "\n",
    "        rs_c_1 = average_pooling(s_1_h.squeeze(0)).squeeze(-1)  # Shape: (786,)\n",
    "        rs_c_2 = average_pooling(s_2_h.squeeze(0)).squeeze(-1)  # Shape: (786,)      \n",
    "\n",
    "        return rs_c_1, rs_c_2\n",
    "    def get_c_router_weights(self, q_c, k_s):\n",
    "        \"\"\"\n",
    "        Compute attention weights using the formula from the paper\n",
    "        score = q_c * k_s / sqrt(d_k)\n",
    "        w = softmax(score)\n",
    "        \"\"\"\n",
    "        # Compute similarity scores\n",
    "        scores = torch.matmul(q_c, k_s.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k).float())\n",
    "        \n",
    "        # Apply softmax to get weights\n",
    "        weights = nn.functional.softmax(scores, dim=-1)\n",
    "        return weights\n",
    "\n",
    "def test_cross_encoder_csr():\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CrossEncoderCSR(model_name=\"roberta-base\")\n",
    "    \n",
    "    # Prepare test inputs\n",
    "    sentences1 = [\"The cat sits on the mat Machine learning is fascinating Hey Hey Hey\"]\n",
    "    sentences2 = [\"A feline rests on a carpet AI is an exciting field Ha Ha Ha\"]\n",
    "    context = [\"pets technology\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    s1_inputs = tokenizer(sentences1, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    s2_inputs = tokenizer(sentences2, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    c_inputs = tokenizer(context, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        s1_hidden, s2_hidden = model(\n",
    "            s1_input_ids=s1_inputs['input_ids'], \n",
    "            s1_attention_mask=s1_inputs['attention_mask'].to(dtype=torch.float),\n",
    "            s2_input_ids=s2_inputs['input_ids'], \n",
    "            s2_attention_mask=s2_inputs['attention_mask'].to(dtype=torch.float),\n",
    "            c_input_ids=c_inputs['input_ids'], \n",
    "            c_attention_mask=c_inputs['attention_mask'].to(dtype=torch.float)\n",
    "        )\n",
    "    \n",
    "    print(\"S1 Hidden States Shape:\", s1_hidden)\n",
    "    print(\"S2 Hidden States Shape:\", s2_hidden)\n",
    "    \n",
    "    # Optional: Compute cosine similarity\n",
    "    def cosine_similarity(a, b):\n",
    "        return torch.nn.functional.cosine_similarity(a, b, dim=-1)\n",
    "    \n",
    "    # Compute and print similarity\n",
    "    for i in range(len(sentences1)):\n",
    "        sim = cosine_similarity(s1_hidden[i], s2_hidden[i])\n",
    "        print(f\"Similarity between '{sentences1[i]}' and '{sentences2[i]}': {sim.item()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_cross_encoder_csr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 Hidden States Shape: torch.Size([1, 768])\n",
      "S2 Hidden States Shape: torch.Size([1, 768])\n",
      "Similarity: 1.000000238418579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class CrossEncoderCSR(nn.Module):\n",
    "    def __init__(self, model_name=\"roberta-base\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize encoders\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.encoder_config = self.encoder.config\n",
    "        self.total_layers = len(self.encoder.encoder.layer)\n",
    "        self.d_k = self.encoder_config.hidden_size\n",
    "\n",
    "        # Linear projections for pseudo query and key\n",
    "        self.W_pseduo_q = nn.Linear(self.d_k, self.d_k)\n",
    "        self.W_pseduo_k = nn.Linear(self.d_k, self.d_k)\n",
    "\n",
    "    def forward(self, s1_input_ids, s1_attention_mask, s2_input_ids, s2_attention_mask, c_input_ids, c_attention_mask):\n",
    "        # Encode context and get [CLS] token embedding\n",
    "        c = self.encoder(c_input_ids, c_attention_mask)\n",
    "        cls = c.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_dim)\n",
    "        q_cls = self.W_pseduo_q(cls)\n",
    "\n",
    "        # Embed sentences using embeddings\n",
    "        s_1_h = self.encoder.embeddings(s1_input_ids)\n",
    "        s_2_h = self.encoder.embeddings(s2_input_ids)\n",
    "\n",
    "        # Process through first half of layers\n",
    "        for i in range(self.total_layers // 2):\n",
    "            layer = self.encoder.encoder.layer[i]\n",
    "            s_1_h = layer(s_1_h, attention_mask=s1_attention_mask.unsqueeze(1).unsqueeze(2))[0]\n",
    "            s_2_h = layer(s_2_h, attention_mask=s2_attention_mask.unsqueeze(1).unsqueeze(2))[0]\n",
    "        \n",
    "        # Project keys and compute router weights\n",
    "        k_s_1 = self.W_pseduo_k(s_1_h)\n",
    "        k_s_2 = self.W_pseduo_k(s_2_h)\n",
    "        w_s_1 = self.get_c_router_weights(q_cls, k_s_1)\n",
    "        w_s_2 = self.get_c_router_weights(q_cls, k_s_2)\n",
    "\n",
    "        # Apply weights\n",
    "        s_1_h = (w_s_1 + 1).transpose(1, 2) * s_1_h\n",
    "        s_2_h = (w_s_2 + 1).transpose(1, 2) * s_2_h\n",
    "\n",
    "        # Process through second half of layers\n",
    "        for i in range(self.total_layers // 2, self.total_layers):\n",
    "            layer = self.encoder.encoder.layer[i]\n",
    "            s_1_h = layer(s_1_h, attention_mask=s1_attention_mask.unsqueeze(1).unsqueeze(2))[0]\n",
    "            s_2_h = layer(s_2_h, attention_mask=s2_attention_mask.unsqueeze(1).unsqueeze(2))[0]\n",
    "\n",
    "        # Apply average pooling\n",
    "        rs_c_1 = s_1_h.mean(dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "        rs_c_2 = s_2_h.mean(dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return rs_c_1, rs_c_2\n",
    "\n",
    "    def get_c_router_weights(self, q_c, k_s):\n",
    "        \"\"\"\n",
    "        Compute attention weights using the formula:\n",
    "        score = q_c * k_s / sqrt(d_k)\n",
    "        w = softmax(score)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(q_c.unsqueeze(1), k_s.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k).float().to(q_c.device))\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        return weights\n",
    "\n",
    "def test_cross_encoder_csr():\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    \n",
    "    # Create model\n",
    "    model = CrossEncoderCSR(model_name=\"roberta-base\")\n",
    "    \n",
    "    # Prepare test inputs\n",
    "    sentences1 = [\"Da Love\"]\n",
    "    sentences2 = [\"Da Love\"]\n",
    "    context = [\"What\"]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    s1_inputs = tokenizer(sentences1, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    s2_inputs = tokenizer(sentences2, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    c_inputs = tokenizer(context, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        s1_hidden, s2_hidden = model(\n",
    "            s1_input_ids=s1_inputs['input_ids'], \n",
    "            s1_attention_mask=s1_inputs['attention_mask'].to(dtype=torch.float),\n",
    "            s2_input_ids=s2_inputs['input_ids'], \n",
    "            s2_attention_mask=s2_inputs['attention_mask'].to(dtype=torch.float),\n",
    "            c_input_ids=c_inputs['input_ids'], \n",
    "            c_attention_mask=c_inputs['attention_mask'].to(dtype=torch.float)\n",
    "        )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"S1 Hidden States Shape:\", s1_hidden.shape)\n",
    "    print(\"S2 Hidden States Shape:\", s2_hidden.shape)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    def cosine_similarity(a, b):\n",
    "        a = a / a.norm(dim=-1, keepdim=True)\n",
    "        b = b / b.norm(dim=-1, keepdim=True)\n",
    "        return torch.sum(a * b, dim=-1)\n",
    "    \n",
    "    sim = cosine_similarity(s1_hidden, s2_hidden)\n",
    "    print(f\"Similarity: {sim.item()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_cross_encoder_csr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
